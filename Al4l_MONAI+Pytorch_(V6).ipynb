{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPyTTwmhDXcuOoB8LH9L0l+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zi-bou/zi-bou/blob/main/Al4l_MONAI%2BPytorch_(V6).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Cardiac MRI Model Evolution Summary\n",
        "\n",
        "| Version | Model                      | Augmentation                         | Loss        | Notes                                                    | Train Acc | Val Acc |\n",
        "|---------|----------------------------|--------------------------------------|-------------|----------------------------------------------------------|-----------|---------|\n",
        "| V1      | ResNet18                   | None                                 | CrossEntropy| Baseline model                                            | 38%       | 30%     |\n",
        "| V2      | ResNet18                   | Basic 3D transforms                  | CrossEntropy| Added transforms to improve robustness                   | 43%       | 20%     |\n",
        "| V3      | ResNet18 + Dropout         | Basic transforms                     | CrossEntropy| Dropout and oversampling added                           | 37%       | 30%     |\n",
        "| V4      | ResNet18 + Dropout         | Class-specific (same for all)        | Focal Loss  | Introduced Focal Loss to handle class imbalance          | 43%       | 30%     |\n",
        "| V5      | DenseNet121 + Dropout      | Class-specific (same for all)        | Focal Loss  | Switched to DenseNet121, achieved better generalization  | 37.5%     | 35%     |\n"
      ],
      "metadata": {
        "id": "DUD5qG-poaTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Cardiac MRI Multi-Task Deep Learning – V6 : Multi-Task Deep Learning Pipeline – (Classification + Segmentation)\n",
        "# BASIC ALGORITHM\n",
        "──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "                    📁 Dataset Folder (on Google Drive)\n",
        "                    └── [pXXXX]/\n",
        "                         ├── frame01.nii          🫀 Diastolic image\n",
        "                         ├── frame11.nii          🫀 Systolic image\n",
        "                         ├── frame01_gt.nii       🎯 Segmentation mask(diastole)\n",
        "                         ├── frame11_gt.nii       🎯 Segmentation mask (systole)\n",
        "                         └── gt.txt               🏷️ Class label (0–4)\n",
        "                         \n",
        "\n",
        "\n",
        "**🔗 Load data from Google Drive**\n",
        "──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "    │ Use `nibabel` to load .nii.gz images\n",
        "    │ Extract 3D volumes: shape = [H, W, D]\n",
        "    │  Read `gt.txt` as classification label\n",
        "    │\n",
        "    │  ➕ Stack frame01 & frame11 → [2, H, W, D]\n",
        "    │  🔄 Permute to PyTorch format → [2, D, H, W]\n",
        "    │\n",
        "    │  Normalize intensities to [0, 1]\n",
        "    │\n",
        "    │  Optional: Apply transforms (flip, zoom, rotate, noise)\n",
        "    │\n",
        "    │  Prepare segmentation masks (diastole & systole):\n",
        "    │    ➕ Stack → [2, D, H, W]\n",
        "    │\n",
        "    └──  - 🎯 Final input:    X = [2, D, H, W]   (2 channels)\n",
        "         -  🎯 Final masks:   Y_seg = [2, D, H, W]\n",
        "         -  🎯 Class label:   Y_cls ∈ [0, 4]\n",
        "\n",
        "\n",
        "**📦 Model Architecture: Multi-Task DenseNet-121**\n",
        "──────────────────────────────────────────────────────────\n",
        "\n",
        "                           Input\n",
        "                      [B, 2, D, H, W]\n",
        "                             │\n",
        "                             ▼\n",
        "                  +----------------------+\n",
        "                  |  Shared Encoder:     |\n",
        "                  |  3D DenseNet-121     |\n",
        "                  +----------------------+\n",
        "                             │\n",
        "             ┌───────────────┴───────────────┐\n",
        "             ▼                               ▼\n",
        "       +--------------------+         +------------------------+\n",
        "       |  Segmentation Head |         |  Classification Head   |\n",
        "       |  Decoder → [B, 2, D, H, W]   |  Linear → [B, 5]       |\n",
        "       +--------------------+         +------------------------+\n",
        "\n",
        "\n",
        "**⚙️ Training Loop**\n",
        "\n",
        "──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "    for epoch in range(N):\n",
        "  \n",
        "      for batch in train_loader:\n",
        "\n",
        "        - Forward pass:\n",
        "            ▸ X → encoder\n",
        "            ▸ Shared features → seg_output & class_output\n",
        "\n",
        "        - Resize ground-truth segmentation masks\n",
        "        - Compute losses:\n",
        "            ▸ Classification loss: Focal Loss\n",
        "            ▸ Segmentation loss: Dice Loss\n",
        "\n",
        "        - Combine losses:\n",
        "            total_loss = λ * Dice + (1 - λ) * Focal\n",
        "\n",
        "        - Backpropagation\n",
        "        - Optimizer step\n",
        "        - Validation + Early Stopping\n",
        "\n",
        "        🔄 Log accuracy & loss per epoch\n",
        "\n",
        "\n",
        "**📊 Evaluation**\n",
        "\n",
        "────────────────────────────────────────────\n",
        "   \n",
        "    - Plot Accuracy / Loss curves\n",
        "\n",
        "    - Show Confusion Matrix\n",
        "\n",
        "    - Display Class Distribution Histogram\n",
        "\n",
        "──────────────────────────────────────────────────────────────\n",
        "\n",
        "💡 **Outcome:**\n",
        "\n",
        "    🔎 Classification: Predicts disease class [0–4]\n",
        "\n",
        "    🎯 Segmentation: Localizes anatomical heart structures\n",
        "\n",
        "──────────────────────────────────────────────────────────────\n"
      ],
      "metadata": {
        "id": "2CfJ_MQSaaJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP BY STEP DETAILED\n",
        "\n",
        "In this project, we build a multi-task learning model that performs two tasks at once:\n",
        "\n",
        "- 🎯 **Classification**: Predict the patient’s cardiac condition  \n",
        "  Output shape: `[B, 5]`  \n",
        "- 🧠 **Segmentation**: Segment anatomical structures (systole + diastole)  \n",
        "  Output shape: `[B, 2, D, H, W]`\n",
        "\n",
        "We use a **3D DenseNet121** as a shared encoder and branch into:\n",
        "\n",
        "- A **classification head**\n",
        "- A **segmentation decoder head**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "AJiH1yO_m764"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#✅ Step 0: Fixing Binary Compatibility\n",
        "\n",
        "On Colab, version mismatches (e.g., numpy, tensorflow, numba) can cause errors.\n",
        "\n",
        "To prevent this:\n",
        "- Uninstall conflicting packages\n",
        "- Install specific versions:  \n",
        "  - `numpy==1.23.5`  \n",
        "  - `monai`, `nibabel`, `matplotlib`, `scikit-learn`, `torch`, etc.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wlqP8y5pnEUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 0: Safe reinstallation to avoid version conflicts\n",
        "# ------------------------------------------\n",
        "# 🧹 Uninstall problematic pre-installed packages in Colab.\n",
        "# These are known to cause binary conflicts when working with MONAI, PyTorch, etc.\n",
        "!pip uninstall -y numpy numba tensorflow thinc\n",
        "\n",
        "# 🧪 Reinstall a compatible version of NumPy for MONAI (1.23.5 works well with PyTorch and MONAI).\n",
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "FmTgJJrG6oRM",
        "outputId": "9b17890f-4f38-4df5-8633-f978561fcc6a",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: thinc 8.3.6\n",
            "Uninstalling thinc-8.3.6:\n",
            "  Successfully uninstalled thinc-8.3.6\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "shap 0.47.1 requires numba>=0.54, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "spacy 3.8.5 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4623d92cc60642398ab4738576da38da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install MONAI (Medical Open Network for AI) and key dependencies for medical imaging and training\n",
        "!pip install monai nibabel matplotlib scikit-learn torch torchvision torchaudio -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD-mwStBnlRs",
        "outputId": "151a25b9-ae45-4c57-cf14-61f8da9afe3a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "shap 0.47.1 requires numba>=0.54, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "spacy 3.8.5 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#✅ Step 1: Mount Google Drive\n",
        "\n",
        "Mount your Google Drive so we can access the dataset.\n",
        "\n",
        "Each patient folder contains:\n",
        "- `frame01.nii`: Systolic volume\n",
        "- `frame11.nii`: Diastolic volume\n",
        "- `frame01_gt.nii`, `frame11_gt.nii`: Corresponding segmentation masks\n",
        "- `gt.txt`: Text file with the patient’s classification label (e.g., “class3”)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "8YuL-hiSnezr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ✅ Step 1: Mount Google Drive\n",
        "# ==========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n6nhdj2WnsCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cd964c-ceaf-4afc-dfc0-a72179c5685d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 2: Imports\n",
        "\n",
        "We import:\n",
        "- `nibabel` to read .nii medical images\n",
        "- `torch` for modeling, training, loss\n",
        "- `monai` for medical models and transforms\n",
        "- `sklearn` for label encoding and class balancing\n",
        "- `matplotlib` for visualizations\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Ayp7B65pn5xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ✅ Step 2: Import required libraries\n",
        "# ==========================================\n",
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We'll use MONAI's DenseNet for the 3D encoder\n",
        "from monai.networks.nets import densenet121\n",
        "# Some MONAI transforms and dice loss for segmentation\n",
        "from monai.transforms import Compose, RandFlip, RandRotate, RandGaussianNoise, RandZoom\n",
        "from monai.losses import DiceLoss"
      ],
      "metadata": {
        "id": "UjMfXFV-n1bQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 3: Estimate Volume Shape\n",
        "\n",
        "We scan one sample image per patient to estimate typical dimensions.\n",
        "\n",
        "- Set target width/height to 224 (for DenseNet compatibility)\n",
        "- Set depth to the 90th percentile of observed depths (or at least 32)\n",
        "\n",
        "Final shape: `[2, 32, 224, 224]`\n",
        "\n",
        "Padding/cropping is applied to match this shape for all volumes.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5BBfqSlWoFRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ✅ Step 3: Estimate volume shape (90th percentile)\n",
        "# ==========================================\n",
        "base_dir = '/content/drive/MyDrive/KAGGLE/Al4l/data/drive-download-20250107T191042Z-001/train'\n",
        "depths, heights, widths = [], [], []\n",
        "\n",
        "# Loop over each patient folder and extract shape from frame01.nii\n",
        "for pid in os.listdir(base_dir):\n",
        "    pdir = os.path.join(base_dir, pid)\n",
        "    if not os.path.isdir(pdir):\n",
        "        continue\n",
        "    files = os.listdir(pdir)\n",
        "    for f in files:\n",
        "        if 'frame01' in f and 'gt' not in f and f.endswith('.nii'):\n",
        "            # Load volume, extract shape\n",
        "            vol = nib.load(os.path.join(pdir, f)).get_fdata()\n",
        "            h, w, d = vol.shape\n",
        "            heights.append(h)\n",
        "            widths.append(w)\n",
        "            depths.append(d)\n",
        "\n",
        "# We'll fix height & width at 224, depth at 90th percentile or at least 32\n",
        "target_height = 224\n",
        "target_width = 224\n",
        "target_depth = max(int(np.percentile(depths, 90)), 32)\n",
        "print(f\"📏 Target shape (H, W, D): {target_height}, {target_width}, {target_depth}\")"
      ],
      "metadata": {
        "id": "fUgwjBYKoJeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d12c48-7bc8-48ce-d55d-b37e8989e263"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📏 Target shape (H, W, D): 224, 224, 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 4: Define Custom PyTorch Dataset\n",
        "\n",
        "We define a `CardiacDataset` class that:\n",
        "- Loads systolic + diastolic volumes and stacks them as channels\n",
        "- Loads both segmentation masks and stacks them too\n",
        "- Pads/crops all data to the fixed shape\n",
        "- Normalizes volume intensities to [0, 1]\n",
        "- Applies optional transforms\n",
        "- Returns:  \n",
        "  - `x`: input volume [2, D, H, W]  \n",
        "  - `y`: class label  \n",
        "  - `seg`: segmentation masks [2, D, H, W]\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lxc5gw8-oWMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ✅ Step 4: Multi-Task Dataset\n",
        "# ==========================================\n",
        "# We'll load:\n",
        "# - Systolic volume (frame01.nii)\n",
        "# - Diastolic volume (another frame e.g. frame11.nii)\n",
        "# - Segmentation mask (both frame01_gt.nii and frame11_gt.nii)\n",
        "# - Class label (from gt.txt)\n",
        "\n",
        "class CardiacDatasetMT(Dataset):\n",
        "    def __init__(self, data_dir, target_shape, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_height, self.target_width, self.target_depth = target_shape\n",
        "        # List of patient folders\n",
        "        self.patient_ids = [pid for pid in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, pid))]\n",
        "        self.labels = []  # textual labels (string)\n",
        "        self.transform = transform\n",
        "\n",
        "        # For each patient, read the classification label from gt.txt\n",
        "        for pid in self.patient_ids:\n",
        "            label_path = os.path.join(data_dir, pid, 'gt.txt')\n",
        "            with open(label_path, 'r') as f:\n",
        "                self.labels.append(f.read().strip())\n",
        "\n",
        "        # Convert textual labels to integer classes\n",
        "        self.encoder = LabelEncoder()\n",
        "        self.encoded_labels = self.encoder.fit_transform(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return total number of patients\n",
        "        return len(self.patient_ids)\n",
        "\n",
        "    def pad_or_crop(self, volume):\n",
        "        \"\"\"\n",
        "        Pad or crop each dimension (H, W, D) to match the target shape\n",
        "        so final shape is [target_height, target_width, target_depth].\n",
        "        We'll do zero-padding if it's smaller.\n",
        "        \"\"\"\n",
        "        h, w, d = volume.shape\n",
        "        pad_h = max(self.target_height - h, 0)\n",
        "        pad_w = max(self.target_width - w, 0)\n",
        "        pad_d = max(self.target_depth - d, 0)\n",
        "\n",
        "        pad = (\n",
        "            (pad_h//2, pad_h - pad_h//2),\n",
        "            (pad_w//2, pad_w - pad_w//2),\n",
        "            (pad_d//2, pad_d - pad_d//2)\n",
        "        )\n",
        "        volume = np.pad(volume, pad, mode='constant')\n",
        "        volume = volume[:self.target_height, :self.target_width, :self.target_depth]\n",
        "        return volume\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a dict:\n",
        "        {\n",
        "          'image': 2-channel volume [2, D, H, W],\n",
        "          'mask': segmentation mask [1, D, H, W],\n",
        "          'class': integer classification label\n",
        "        }\n",
        "        \"\"\"\n",
        "        pid = self.patient_ids[idx]\n",
        "        pdir = os.path.join(self.data_dir, pid)\n",
        "        files = os.listdir(pdir)\n",
        "\n",
        "        # Identify files for systolic, diastolic, and segmentation\n",
        "        systolic, diastolic = None, None\n",
        "        seg1_file, seg2_file = None, None\n",
        "\n",
        "        for f in files:\n",
        "            if 'frame01' in f and 'gt' not in f and f.endswith('.nii'):\n",
        "                systolic = f\n",
        "            elif 'frame' in f and 'gt' not in f and 'frame01' not in f and f.endswith('.nii'):\n",
        "                diastolic = f\n",
        "            elif 'frame01_gt' in f and f.endswith('.nii'):\n",
        "                seg1_file = f\n",
        "            elif 'frame11_gt' in f and f.endswith('.nii'):\n",
        "                seg2_file = f\n",
        "\n",
        "        # Load volumes from .nii\n",
        "        syst_data = nib.load(os.path.join(pdir, systolic)).get_fdata() if systolic else None\n",
        "        diast_data = nib.load(os.path.join(pdir, diastolic)).get_fdata() if diastolic else None\n",
        "\n",
        "        # Pad/crop both volumes\n",
        "        syst_data = self.pad_or_crop(syst_data)\n",
        "        diast_data = self.pad_or_crop(diast_data)\n",
        "\n",
        "        # Stack as channels => shape [2, H, W, D]\n",
        "        combined = np.stack((syst_data, diast_data), axis=0)\n",
        "        # Normalize intensities to [0,1]\n",
        "        combined = combined / np.max(combined)\n",
        "\n",
        "        # Convert to torch tensor => [2, H, W, D] => permute to [2, D, H, W]\n",
        "        combined = torch.tensor(combined, dtype=torch.float32)\n",
        "        combined = combined.permute(0, 3, 1, 2)\n",
        "\n",
        "        # Load segmentation masks and stack them\n",
        "        if seg1_file and seg2_file:\n",
        "            seg1 = nib.load(os.path.join(pdir, seg1_file)).get_fdata()\n",
        "            seg2 = nib.load(os.path.join(pdir, seg2_file)).get_fdata()\n",
        "\n",
        "            seg1 = self.pad_or_crop(seg1)\n",
        "            seg2 = self.pad_or_crop(seg2)\n",
        "\n",
        "            # Convert any label > 0 => 1 (binary mask)\n",
        "            seg1 = (seg1 > 0).astype(np.float32)\n",
        "            seg2 = (seg2 > 0).astype(np.float32)\n",
        "\n",
        "            # Combine the two binary masks by max (union)\n",
        "            combined_seg = np.maximum(seg1, seg2)\n",
        "            seg_data = torch.tensor(combined_seg, dtype=torch.float32)\n",
        "            seg_data = seg_data.permute(2, 0, 1).unsqueeze(0)  # [1, D, H, W]\n",
        "\n",
        "        else:\n",
        "            # Fill with zeros if missing masks\n",
        "            seg_data = torch.zeros((1, self.target_depth, self.target_height, self.target_width), dtype=torch.float32)\n",
        "\n",
        "        # Apply augmentation only to the image (not mask)\n",
        "        if self.transform:\n",
        "            combined = self.transform(combined)\n",
        "\n",
        "        class_label = self.encoded_labels[idx]\n",
        "\n",
        "        return {\n",
        "            \"image\": combined,     # [2, D, H, W]\n",
        "            \"mask\": seg_data,      # [1, D, H, W]\n",
        "            \"class\": class_label   # int\n",
        "        }\n"
      ],
      "metadata": {
        "id": "uh9VE5nVoa_k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 5: Data Augmentation (Optional)\n",
        "\n",
        "We define random transformations:\n",
        "- Flips\n",
        "- Rotations\n",
        "- Zoom\n",
        "- Gaussian noise\n",
        "\n",
        "These help improve generalization.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Mgwr8lMoogaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ✅ Step 5: Class-specific data augmentations\n",
        "# ============================\n",
        "base_transform = Compose([\n",
        "    RandFlip(spatial_axis=0, prob=0.5),\n",
        "    RandRotate(range_x=0.1, prob=0.5),\n",
        "    RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
        "    RandGaussianNoise(prob=0.3, mean=0.0, std=0.05)\n",
        "])\n",
        "\n",
        "class CardiacDatasetWithClassAugmentMT(CardiacDatasetMT):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # We'll map class -> transform so each class can get specialized transforms\n",
        "        # For now, all are the same\n",
        "        self.class_transforms = {i: base_transform for i in range(5)}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_dict = super().__getitem__(idx)\n",
        "        label = data_dict[\"class\"]\n",
        "        transform = self.class_transforms.get(label, None)\n",
        "        if transform:\n",
        "            # Apply transform only on \"image\"\n",
        "            data_dict[\"image\"] = transform(data_dict[\"image\"])\n",
        "        return data_dict"
      ],
      "metadata": {
        "id": "LMsB6KWRolbW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 6: Dataset Preparation and Class Balancing\n",
        "\n",
        "- Apply weighted sampling to address class imbalance\n",
        "- Create training and validation loaders\n",
        "- Use `WeightedRandomSampler` for training\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "M1YT1pSLosxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ✅ Step 6: Dataset prep, Weighted Sampling\n",
        "# ============================\n",
        "target_shape = (target_height, target_width, target_depth)\n",
        "dataset = CardiacDatasetWithClassAugmentMT(base_dir, target_shape)\n",
        "\n",
        "# Compute class weights for classification\n",
        "y_all = [dataset.encoded_labels[i] for i in range(len(dataset))]\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_all), y=y_all)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Weighted sampler for training\n",
        "train_indices = train_ds.indices if hasattr(train_ds, 'indices') else list(range(len(train_ds)))\n",
        "train_labels = [dataset.encoded_labels[i] for i in train_indices]\n",
        "train_weights = [class_weights[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(train_weights, num_samples=len(train_weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=2, sampler=sampler)\n",
        "val_loader = DataLoader(val_ds, batch_size=2)\n",
        "\n",
        "# Free up any leftover GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "G8qgJm1Tot5c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ✅ Step 7: Define Model\n",
        "\n",
        "We use a modified `DenseNet121` from MONAI:\n",
        "\n",
        "- Shared 3D encoder\n",
        "- Two heads:\n",
        "  - One for classification (`Linear(1024, 5)`)\n",
        "  - One for segmentation (`ConvTranspose3d` decoder)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "l_s48kXRo5Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ✅ Step 7: Multi-task DenseNet\n",
        "# ============================\n",
        "# We'll define a single 3D DenseNet as encoder,\n",
        "# then produce 2 outputs: class_out (5 classes), seg_out (1 channel).\n",
        "\n",
        "\n",
        "# ✅ Multi-task model: classification + segmentation\n",
        "class MultiTaskDenseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Shared encoder\n",
        "        base_model = densenet121(spatial_dims=3, in_channels=2, out_channels=5)\n",
        "        self.backbone = base_model.features  # Just the feature extractor\n",
        "\n",
        "        # Classifier head (after global pooling)\n",
        "        self.class_dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(1024, 5)\n",
        "\n",
        "        # Segmentation head (upconvolution to full resolution)\n",
        "        self.seg_head = nn.Sequential(\n",
        "            nn.ConvTranspose3d(1024, 512, kernel_size=2, stride=2),  # Upsample\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(256, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(64, 1, kernel_size=2, stride=2),      # Output 1 channel mask\n",
        "            nn.Sigmoid()  # Since we use DiceLoss\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"[DEBUG] Input:\", x.shape)  # [B, 2, 32, 224, 224]\n",
        "        x = self.backbone(x)              # [B, 1024, D', H', W'] → e.g., [B, 1024, 2, 14, 14]\n",
        "        print(\"[DEBUG] Features:\", x.shape)\n",
        "\n",
        "        # Classification head\n",
        "        x_class = F.adaptive_avg_pool3d(x, (1, 1, 1))  # [B, 1024, 1, 1, 1]\n",
        "        x_class = torch.flatten(x_class, 1)            # [B, 1024]\n",
        "        x_class = self.class_dropout(x_class)\n",
        "        x_class = self.classifier(x_class)             # [B, 5]\n",
        "\n",
        "        # Segmentation head\n",
        "        x_seg = self.seg_head(x)                       # [B, 1, 32, 224, 224] ideally\n",
        "        print(\"[DEBUG] Segmentation output:\", x_seg.shape)\n",
        "\n",
        "        return x_class, x_seg\n",
        "\n",
        "model = MultiTaskDenseNet().to('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "3NizStWUpBTb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 8: Define Loss Function\n",
        "\n",
        "We combine:\n",
        "- `FocalLoss` for classification\n",
        "- `DiceLoss` for segmentation\n",
        "\n",
        "The total loss = weighted sum of both.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IRJflB0IpIVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ✅ Step 8: Define losses & optimizer\n",
        "# ============================\n",
        "# We'll combine:\n",
        "# 1) Focal Loss for classification\n",
        "# 2) Dice Loss (sigmoid) for segmentation (binary)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
        "\n",
        "focal_loss = FocalLoss(alpha=class_weights_tensor, gamma=2.0)\n",
        "dice_loss = DiceLoss(sigmoid=True)  # binary segmentation => use sigmoid\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "def multi_task_loss_fn(class_pred, class_label, seg_pred, seg_mask):\n",
        "    \"\"\"\n",
        "    Combine classification + segmentation losses into a single total_loss.\n",
        "    - class_pred: [B, 5]  => for classification\n",
        "    - class_label: [B]    => integer class\n",
        "    - seg_pred: [B, 1, D', H', W'] => raw logits for seg\n",
        "    - seg_mask: [B, D', H', W']    => binary target\n",
        "    \"\"\"\n",
        "    # classification loss\n",
        "    c_loss = focal_loss(class_pred, class_label)\n",
        "\n",
        "    # segmentation: need channels => seg_mask => [B, 1, D', H', W']\n",
        "    seg_mask = seg_mask.unsqueeze(1)\n",
        "    s_loss = dice_loss(seg_pred, seg_mask)\n",
        "\n",
        "    # Weighted sum of both\n",
        "    total = c_loss + 0.5 * s_loss\n",
        "    return total, c_loss.item(), s_loss.item()"
      ],
      "metadata": {
        "id": "sHDzqjRipLVY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 9: Training Loop\n",
        "\n",
        "- Train for up to 30 epochs\n",
        "- Track training and validation accuracy/loss\n",
        "- Use `ReduceLROnPlateau` to reduce LR on stagnating validation loss\n",
        "- Use early stopping to save the best model\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wL3t6EXMpNZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ✅ Step 9: Training Loop for Multi-Task Learning\n",
        "# ============================\n",
        "train_acc_list, val_acc_list = [], []\n",
        "train_loss_list, val_loss_list = [], []\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "for epoch in range(1, 31):\n",
        "    model.train()\n",
        "    running_loss, correct = 0.0, 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[\"image\"].to(device)       # [B, 2, D, H, W]\n",
        "        seg_masks = batch[\"mask\"].to(device)     # [B, D, H, W]\n",
        "        labels = batch[\"class\"].to(device)       # [B]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        class_out, seg_out = model(inputs)\n",
        "\n",
        "        # 🛠️ Ensure segmentation masks are in shape [B, 1, D, H, W] before resizing\n",
        "        if seg_masks.ndim == 4:\n",
        "            seg_masks = seg_masks.unsqueeze(1)  # [B, D, H, W] → [B, 1, D, H, W]\n",
        "        elif seg_masks.ndim == 6:\n",
        "            seg_masks = seg_masks.squeeze(1)    # Remove singleton if shape is [B, 1, 1, D, H, W]\n",
        "\n",
        "        print(\"[DEBUG] Input to model:\", inputs.shape)\n",
        "        print(\"[DEBUG] Classification output:\", class_out.shape)\n",
        "        print(\"[DEBUG] Raw segmentation output:\", seg_out.shape)\n",
        "        print(\"[DEBUG] Ground truth seg_masks shape before resize:\", seg_masks.shape)\n",
        "\n",
        "        # Resize seg_masks to match seg_out shape\n",
        "        seg_masks_resized = F.interpolate(seg_masks, size=seg_out.shape[2:], mode='trilinear', align_corners=False)\n",
        "        seg_masks_resized = seg_masks_resized.squeeze(2)  # Remove the singleton channel dimension\n",
        "\n",
        "\n",
        "        # Final shape check before loss\n",
        "        if seg_masks_resized.shape != seg_out.shape:\n",
        "            print(\"[DEBUG] Shape mismatch before loss\")\n",
        "            print(f\"  -> seg_out: {seg_out.shape}\")\n",
        "            print(f\"  -> seg_masks_resized: {seg_masks_resized.shape}\")\n",
        "            raise ValueError(\"Mismatch between segmentation prediction and target shapes\")\n",
        "\n",
        "        print(\"[DEBUG] Resized segmentation mask:\", seg_masks_resized.shape)\n",
        "\n",
        "        # Compute joint loss (classification + segmentation)\n",
        "        loss, c_l, s_l = multi_task_loss_fn(class_out, labels, seg_out, seg_masks_resized)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        preds = class_out.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    train_acc = correct / total_samples\n",
        "    train_acc_list.append(train_acc)\n",
        "    train_loss_list.append(running_loss / len(train_loader))\n",
        "\n",
        "    # =====================\n",
        "    # 🔍 Validation Phase\n",
        "    # =====================\n",
        "    model.eval()\n",
        "    val_correct, val_loss_epoch = 0, 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs = batch[\"image\"].to(device)\n",
        "            seg_masks = batch[\"mask\"].to(device)\n",
        "            labels = batch[\"class\"].to(device)\n",
        "\n",
        "            class_out, seg_out = model(inputs)\n",
        "\n",
        "            # Same preprocessing for val seg masks\n",
        "            if seg_masks.ndim == 4:\n",
        "                seg_masks = seg_masks.unsqueeze(1)\n",
        "            elif seg_masks.ndim == 6:\n",
        "                seg_masks = seg_masks.squeeze(1)\n",
        "\n",
        "            seg_masks_resized = F.interpolate(seg_masks, size=seg_out.shape[2:], mode='trilinear', align_corners=False)\n",
        "\n",
        "            # Final shape check\n",
        "            if seg_masks_resized.shape != seg_out.shape:\n",
        "                print(\"[DEBUG] Validation shape mismatch\")\n",
        "                print(f\"  -> seg_out: {seg_out.shape}\")\n",
        "                print(f\"  -> seg_masks_resized: {seg_masks_resized.shape}\")\n",
        "                raise ValueError(\"Validation mismatch between segmentation prediction and target shapes\")\n",
        "\n",
        "            loss, c_l, s_l = multi_task_loss_fn(class_out, labels, seg_out, seg_masks_resized)\n",
        "\n",
        "            val_loss_epoch += loss.item()\n",
        "            preds = class_out.argmax(dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = val_correct / len(val_ds)\n",
        "    val_acc_list.append(val_acc)\n",
        "    val_loss_epoch = val_loss_epoch / len(val_loader)\n",
        "    val_loss_list.append(val_loss_epoch)\n",
        "    scheduler.step(val_loss_epoch)\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}, Train Loss = {train_loss_list[-1]:.4f}, Val Loss = {val_loss_epoch:.4f}\")\n",
        "\n",
        "    if val_loss_epoch < best_val_loss:\n",
        "        best_val_loss = val_loss_epoch\n",
        "        early_stop_counter = 0\n",
        "        torch.save(model.state_dict(), \"/content/best_model_densenet121_v6.pth\")\n",
        "        print(\"✅ Saved new best model\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= 5:\n",
        "            print(\"⏹️ Early stopping triggered.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "gFFIfhDtpP3H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "4bab41bd-222c-4f5a-8544-118e727a59f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Input: torch.Size([2, 2, 32, 224, 224])\n",
            "[DEBUG] Features: torch.Size([2, 1024, 1, 7, 7])\n",
            "[DEBUG] Segmentation output: torch.Size([2, 1, 16, 112, 112])\n",
            "[DEBUG] Input to model: torch.Size([2, 2, 32, 224, 224])\n",
            "[DEBUG] Classification output: torch.Size([2, 5])\n",
            "[DEBUG] Raw segmentation output: torch.Size([2, 1, 16, 112, 112])\n",
            "[DEBUG] Ground truth seg_masks shape before resize: torch.Size([2, 1, 32, 224, 224])\n",
            "[DEBUG] Resized segmentation mask: torch.Size([2, 1, 16, 112, 112])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ground truth has different shape (torch.Size([2, 1, 1, 16, 112, 112])) from input (torch.Size([2, 1, 16, 112, 112]))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9788636c0f21>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Compute joint loss (classification + segmentation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_task_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_masks_resized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8f350423d2f2>\u001b[0m in \u001b[0;36mmulti_task_loss_fn\u001b[0;34m(class_pred, class_label, seg_pred, seg_mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# segmentation: need channels => seg_mask => [B, 1, D', H', W']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mseg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0ms_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Weighted sum of both\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/losses/dice.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ground truth has different shape ({target.shape}) from input ({input.shape})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# reducing only spatial dimensions (not batch nor channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: ground truth has different shape (torch.Size([2, 1, 1, 16, 112, 112])) from input (torch.Size([2, 1, 16, 112, 112]))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Step 10: Evaluation\n",
        "\n",
        "We visualize:\n",
        "- Accuracy and loss curves\n",
        "- Confusion matrix\n",
        "- Class distribution histogram\n",
        "\n",
        "Model is saved to:  \n",
        "`/content/best_model_densenet121_v6.pth`\n"
      ],
      "metadata": {
        "id": "_lHu0ZXlW1im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ✅ Step 10: Visualization - learning curves + confusion matrix\n",
        "# ============================\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_acc_list, label='Train Accuracy')\n",
        "plt.plot(val_acc_list, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Multi-task Accuracy Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_loss_list, label='Train Loss')\n",
        "plt.plot(val_loss_list, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Multi-task Loss Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "display.plot(cmap='Blues')\n",
        "plt.title(\"Validation Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Best model saved at: /content/best_model_densenet121_v6.pth\")\n",
        "print(\"✅ Multi-task learning completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "QOCyn8IW6SQ5",
        "outputId": "704f898d-b8d8-4872-c64b-d18c596f0c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "📏 Target shape (H, W, D): 224, 224, 32\n",
            "[DEBUG] Input: torch.Size([2, 2, 32, 224, 224])\n",
            "[DEBUG] Features: torch.Size([2, 1024, 1, 7, 7])\n",
            "[DEBUG] Segmentation output: torch.Size([2, 1, 16, 112, 112])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ground truth has different shape (torch.Size([2, 1, 32, 16, 112, 112])) from input (torch.Size([2, 1, 16, 112, 112]))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-72db2f5e9abe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# Compute combined loss using resized ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_task_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_masks_resized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-72db2f5e9abe>\u001b[0m in \u001b[0;36mmulti_task_loss_fn\u001b[0;34m(class_pred, class_label, seg_pred, seg_mask)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# segmentation: need channels => seg_mask => [B, 1, D', H', W']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mseg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0ms_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# Weighted sum of both\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/monai/losses/dice.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ground truth has different shape ({target.shape}) from input ({input.shape})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# reducing only spatial dimensions (not batch nor channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: ground truth has different shape (torch.Size([2, 1, 32, 16, 112, 112])) from input (torch.Size([2, 1, 16, 112, 112]))"
          ]
        }
      ]
    }
  ]
}